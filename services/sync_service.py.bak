"""
SERVICE: SYNC SERVICE (FINAL SELF-CONTAINED FIX)
------------------------------------------------
1. Path Aware: Captures 'Brand | Model' properly.
2. Direct API Calls: Bypasses missing methods in DriveManager.
3. Update Only: Updates existing 'drive_index.json' to avoid Quota limits.
4. METADATA EXTRACTION: Extracts Brand, Model, and Meta_Type from file paths.
"""
import streamlit as st
import json
import os
from core.drive_manager import DriveManager
from core.config_loader import ConfigLoader
import logging
from googleapiclient.http import MediaIoBaseUpload
import io
import re

logger = logging.getLogger("Sync")
INDEX_FILENAME = "drive_index.json"

class SyncService:
    def __init__(self):
        self.drive = DriveManager()
        self.root_id = ConfigLoader.get_drive_folder_id()

    def scan_library(self):
        """Œ£Œ±œÅœéŒΩŒµŒπ Œ∫Œ±Œπ ŒïŒùŒóŒúŒïŒ°Œ©ŒùŒïŒô (Update) œÑŒø Œ±œÅœáŒµŒØŒø œÉœÑŒø Cloud."""
        logger.info("üîÑ Starting Sync (Direct Mode)...")
        
        if not self.root_id: 
            logger.error("‚ùå Root ID missing.")
            return []
        
        # ŒúœÄŒ¨œÅŒ± Œ†œÅŒøœåŒ¥ŒøœÖ
        progress_text = "‚è≥ Œ£Œ¨œÅœâœÉŒ∑ & ŒïŒΩŒ∑ŒºŒ≠œÅœâœÉŒ∑..."
        my_bar = st.progress(0, text=progress_text)
        
        # 1. Œ£ŒëŒ°Œ©Œ£Œó (Path Aware)
        # Œ†ŒµœÅŒΩŒ¨ŒºŒµ œÑŒø my_bar Œ∫Œ±Œπ progress_text Œ≥ŒπŒ± ŒµŒΩŒ∑ŒºŒµœÅœéœÉŒµŒπœÇ œÉœÑŒ∑ŒΩ œÄœÅœåŒøŒ¥Œø
        # Adjusted total_progress_steps to allow for better granularity, assuming ~100 files in a small library
        all_files = self._scan_recursive(self.root_id, path_prefix="", my_bar=my_bar, progress_text=progress_text, current_progress=0, total_progress_steps=80) # 80% for scanning
        
        my_bar.progress(80, text=f"‚úÖ ŒíœÅŒ≠Œ∏Œ∑Œ∫Œ±ŒΩ {len(all_files)} Œ±œÅœáŒµŒØŒ±. ŒïŒ≥Œ≥œÅŒ±œÜŒÆ œÉœÑŒø Cloud...")
        logger.info(f"‚úÖ Scan Complete. Found {len(all_files)} manuals.")

        # 2. ŒëœÄŒøŒ∏ŒÆŒ∫ŒµœÖœÉŒ∑ Œ§ŒøœÄŒπŒ∫Œ¨ (Backup)
        try:
            with open(INDEX_FILENAME, "w", encoding="utf-8") as f:
                json.dump(all_files, f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ Local index saved: {INDEX_FILENAME}")
        except Exception as e:
            logger.warning(f"Failed to save local index: {e}")

        # 3. CLOUD UPDATE (Direct API Call - ŒßœâœÅŒØœÇ ŒºŒµœÉŒ¨Œ∂ŒøŒΩœÑŒµœÇ)
        try:
            # ŒëœÄŒµœÖŒ∏ŒµŒØŒ±œÇ Œ±ŒΩŒ±Œ∂ŒÆœÑŒ∑œÉŒ∑ ŒºŒ≠œÉœâ œÑŒøœÖ service (œÄŒ±œÅŒ±Œ∫Œ¨ŒºœÄœÑŒøœÖŒºŒµ œÑŒø DriveManager)
            query = f"name = '{INDEX_FILENAME}' and '{self.root_id}' in parents and trashed = false"
            results = self.drive.service.files().list(q=query, fields="files(id, name)").execute()
            found_files = results.get('files', [])
            
            if not found_files:
                logger.error("‚ùå CLOUD ERROR: ŒîŒµŒΩ Œ≤œÅŒ≠Œ∏Œ∑Œ∫Œµ œÑŒø 'drive_index.json'!")
                st.error("‚ö†Ô∏è Œ£œÜŒ¨ŒªŒºŒ±: Œ†œÅŒ≠œÄŒµŒπ ŒΩŒ± Œ¥Œ∑ŒºŒπŒøœÖœÅŒ≥ŒÆœÉŒµœÑŒµ Œ≠ŒΩŒ± Œ∫ŒµŒΩœå Œ±œÅœáŒµŒØŒø 'drive_index.json' œÉœÑŒø Drive œÉŒ±œÇ!")
                return all_files

            # Œ†Œ±ŒØœÅŒΩŒøœÖŒºŒµ œÑŒø ID œÑŒøœÖ Œ±œÅœáŒµŒØŒøœÖ
            target_file_id = found_files[0]['id']
            logger.info(f"üìÇ Found Cloud Index ID: {target_file_id}")
            
            # ŒïœÑŒøŒπŒºŒ¨Œ∂ŒøœÖŒºŒµ œÑŒ± Œ¥ŒµŒ¥ŒøŒºŒ≠ŒΩŒ±
            json_str = json.dumps(all_files, ensure_ascii=False, indent=2)
            media = MediaIoBaseUpload(io.BytesIO(json_str.encode('utf-8')), mimetype='application/json', resumable=True)

            # ŒïŒöŒ§ŒïŒõŒïŒ£Œó UPDATE
            self.drive.service.files().update(
                fileId=target_file_id,
                media_body=media
            ).execute()
            
            logger.info(f"‚òÅÔ∏è Cloud Index OVERWRITTEN successfully!")
            my_bar.progress(100, text="‚úÖ ŒüŒªŒøŒ∫ŒªŒ∑œÅœéŒ∏Œ∑Œ∫Œµ! Œó Œ≤Œ¨œÉŒ∑ ŒµŒΩŒ∑ŒºŒµœÅœéŒ∏Œ∑Œ∫Œµ.")
            
            # ŒöŒ±Œ∏Œ±œÅŒπœÉŒºœåœÇ Session
            if 'library_index' in st.session_state:
                del st.session_state['library_index']
            if 'library_cache' in st.session_state: # Clear cache from ui_search
                del st.session_state['library_cache']
                
            return all_files
            
        except Exception as e:
            logger.error(f"‚ùå Cloud Update Failed: {e}", exc_info=True)
            st.error(f"‚ùå Œ£œÜŒ¨ŒªŒºŒ± Œ∫Œ±œÑŒ¨ œÑŒ∑ŒΩ ŒµŒΩŒ∑ŒºŒ≠œÅœâœÉŒ∑ Cloud: {e}")
            return all_files

    def _extract_metadata_from_name(self, full_path_name: str, original_filename: str) -> dict:
        """
        ŒïŒæŒ¨Œ≥ŒµŒπ ŒºŒµœÑŒ±Œ¥ŒµŒ¥ŒøŒºŒ≠ŒΩŒ± (brand, model, meta_type, error_codes) Œ±œÄœå Œ≠ŒΩŒ± œåŒΩŒøŒºŒ± Œ±œÅœáŒµŒØŒøœÖ
        œÄŒøœÖ Œ≠œáŒµŒπ ŒºŒøœÅœÜŒøœÄŒøŒπŒ∑Œ∏ŒµŒØ Œ±œÄœå œÑŒøŒΩ Sorter (œÄ.œá., "Category | Brand | Model | Type | Filename.pdf")
        ŒÆ Œ±œÄœå œÑŒ∑ŒΩ original_filename Œ±ŒΩ Œ¥ŒµŒΩ œÖœÄŒ¨œÅœáŒµŒπ œÄŒªŒÆœÅŒ∑œÇ Œ¥ŒπŒ±Œ¥œÅŒøŒºŒÆ.
        """
        metadata = {
            'brand': 'Unknown',
            'model': 'General Model',
            'meta_type': 'DOC',
            'error_codes': '', # Requires AI to extract, for now remains empty from path
            'original_name': original_filename # Keep the original filename
        }

        # Œ†œÅŒøœÉœÄŒ±Œ∏ŒøœçŒºŒµ ŒΩŒ± Œ¥ŒπŒ±œÉœÄŒ¨œÉŒøœÖŒºŒµ ŒºŒµ Œ≤Œ¨œÉŒ∑ œÑŒø '|' Œ±œÄœå œÑŒ∑ŒΩ œÄŒªŒÆœÅŒ∑ Œ¥ŒπŒ±Œ¥œÅŒøŒºŒÆ
        parts = [p.strip() for p in full_path_name.split('|')]
        
        # ŒëŒΩ Œ∑ œÄŒªŒÆœÅŒ∑œÇ Œ¥ŒπŒ±Œ¥œÅŒøŒºŒÆ ŒµŒØŒΩŒ±Œπ ŒºŒøœÅœÜŒøœÄŒøŒπŒ∑ŒºŒ≠ŒΩŒ∑ (œÄ.œá., Category | Brand | Model | Type)
        # The Sorter's naming convention for a file within its full path is:
        # Category | Brand | Model | Type | Original_filename_without_ext.pdf
        # So, if we split the full_path_name by '|', we expect at least 4 parts before the filename itself (which is the last part).
        if len(parts) >= 4: 
            # metadata['category'] = parts[0] # Not directly needed for search, but good to know
            metadata['brand'] = parts[1] if parts[1] else 'Unknown'
            metadata['model'] = parts[2] if parts[2] else 'General Model'
            metadata['meta_type'] = parts[3] if parts[3] else 'DOC'
            # The actual file name is often the last part, after potential type info
            # The 'name' field in the item already contains the full path name, so 'original_name' is distinct.

        # Fallback for brand/model/type if not found in path_prefix, try to infer from original_filename
        # (e.g. if the file wasn't sorted yet, but we want some basic metadata)
        if metadata['brand'] == 'Unknown' and original_filename:
            match = re.search(r'^(.*?)[_ -](.*?)', original_filename, re.IGNORECASE)
            if match:
                metadata['brand'] = match.group(1).strip()
                # A very basic attempt to get a model from the first few words
                if len(match.group(2)) > 3:
                     metadata['model'] = match.group(2).split(' ')[0].strip()

        # Basic error code extraction from original filename if not already set
        if not metadata['error_codes'] and original_filename:
            error_match = re.search(r'(E\d+|ERROR\s*\d+)', original_filename, re.IGNORECASE)
            if error_match:
                metadata['error_codes'] = error_match.group(0).upper()
        
        return metadata


    def _scan_recursive(self, folder_id, path_prefix="", my_bar=None, progress_text="", current_progress=0, total_progress_steps=0, depth=0):
        """
        ŒßœÑŒØŒ∂ŒµŒπ œÑŒ± ŒøŒΩœåŒºŒ±œÑŒ± ŒºŒµ Œ≤Œ¨œÉŒ∑ œÑŒøœÖœÇ œÜŒ±Œ∫Œ≠ŒªŒøœÖœÇ (ARISTON | FAST...).
        Œ†ŒµœÅŒπŒøœÅŒØŒ∂ŒµŒπ œÑŒ∑ œÉŒ¨œÅœâœÉŒ∑ œÉŒµ œÉœÖŒ≥Œ∫ŒµŒ∫œÅŒπŒºŒ≠ŒΩŒ± Œ≤Œ¨Œ∏Œ∑ Œ∫Œ±Œπ œÄŒ±œÅŒ±Œ∫Œ¨ŒºœÄœÑŒµŒπ œÑŒøœÖœÇ œÑŒ±ŒæŒπŒΩŒøŒºŒ∑ŒºŒ≠ŒΩŒøœÖœÇ œÜŒ±Œ∫Œ≠ŒªŒøœÖœÇ
        Œ≥ŒπŒ± ŒΩŒ± ŒºŒ∑ŒΩ ŒµœÄŒµŒæŒµœÅŒ≥Œ¨Œ∂ŒøŒΩœÑŒ±Œπ Œ¥ŒπœÄŒªŒ¨.
        """
        results = []
        if depth > 5: # Œ†ŒµœÅŒπŒøœÅŒπœÉŒºœåœÇ Œ≤Œ¨Œ∏ŒøœÖœÇ Œ≥ŒπŒ± Œ±œÄŒøœÜœÖŒ≥ŒÆ Œ¨œÄŒµŒπœÅŒ∑œÇ Œ±ŒΩŒ±Œ¥œÅŒøŒºŒÆœÇ
            logger.warning(f"Reached max recursion depth for folder {folder_id}.")
            return [] 

        try:
            items = self.drive.list_files_in_folder(folder_id)
            
            subfolders = [i for i in items if i['mimeType'] == 'application/vnd.google-apps.folder']
            files = [i for i in items if i['mimeType'] == 'application/pdf']
            
            # Dynamic progress increment for this level
            remaining_progress = total_progress_steps - current_progress
            total_items_in_level = len(files) + len(subfolders)
            progress_for_level = remaining_progress / total_items_in_level if total_items_in_level > 0 else 0

            # Files
            for i, item in enumerate(files):
                display_name = item['name']
                if path_prefix:
                    display_name = f"{path_prefix} | {item['name']}"
                
                meta = {
                    'file_id': item['id'],
                    'name': display_name,
                    'link': item.get('webViewLink', ''),
                    'mime': item['mimeType']
                }
                
                # ŒùŒïŒü: ŒïŒæŒ±Œ≥œâŒ≥ŒÆ œÄœÅœåœÉŒ∏ŒµœÑœâŒΩ ŒºŒµœÑŒ±Œ¥ŒµŒ¥ŒøŒºŒ≠ŒΩœâŒΩ
                # Pass original item name for better metadata extraction
                extracted_meta = self._extract_metadata_from_name(display_name, item['name'])
                meta.update(extracted_meta)
                
                results.append(meta)
                if my_bar:
                    current_progress += progress_for_level / total_items_in_level
                    my_bar.progress(min(100, int(current_progress)), text=f"{progress_text} - Scanning: {item['name']}")

            # Folders
            for i, folder in enumerate(subfolders):
                # Skip folders that are explicitly ignored or represent already categorized branches
                # This logic should match the Sorter's logic for filtering what to process.
                # For sync service, we want to index everything regardless of being sorted,
                # but we still want to skip actual trash folders or manual review zones.
                if folder['name'] in ["Trash", "_MANUAL_REVIEW", "_AI_ERROR", "_NEEDS_OCR"]: # Add other ignored folders here
                    logger.info(f"üìÇ Skipping ignored folder during sync scan: {folder['name']}")
                    continue

                new_prefix = folder['name']
                if path_prefix:
                    new_prefix = f"{path_prefix} | {folder['name']}"
                
                # Update progress before recursive call
                if my_bar:
                    my_bar.progress(min(100, int(current_progress + (i / len(subfolders)) * progress_for_level)), text=f"{progress_text} - Entering folder: {folder['name']}")

                results.extend(self._scan_recursive(folder['id'], new_prefix, my_bar, progress_text, current_progress, total_progress_steps, depth + 1))
                
        except Exception as e:
            logger.warning(f"Scan Error in folder {folder_id} with prefix '{path_prefix}': {e}", exc_info=True)
        return results

    def load_index(self):
        # 1. Local
        if os.path.exists(INDEX_FILENAME):
            try:
                with open(INDEX_FILENAME, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    logger.info(f"üíæ Loaded local index: {len(data)} files.")
                    return data
            except Exception as e:
                logger.warning(f"Failed to load local index: {e}")
            
        # 2. Cloud Download (Direct API)
        if self.root_id:
            try:
                query = f"name = '{INDEX_FILENAME}' and '{self.root_id}' in parents and trashed = false"
                results = self.drive.service.files().list(q=query, fields="files(id)").execute()
                files = results.get('files', [])
                
                if files:
                    file_id = files[0]['id']
                    stream = self.drive.download_file_content(file_id)
                    if stream:
                        data = json.load(stream)
                        # Cache
                        with open(INDEX_FILENAME, "w", encoding="utf-8") as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        logger.info(f"‚òÅÔ∏è Downloaded and cached cloud index: {len(data)} files.")
                        return data
            except Exception as e:
                logger.error(f"‚ùå Cloud Index Download Failed: {e}", exc_info=True)
        return []