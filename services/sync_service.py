"""
SERVICE: SYNC SERVICE (FINAL SELF-CONTAINED FIX)
------------------------------------------------
1. Path Aware: Captures 'Brand | Model' properly.
2. Direct API Calls: Bypasses missing methods in DriveManager.
3. Update Only: Updates existing 'drive_index.json' to avoid Quota limits.
4. METADATA EXTRACTION: Extracts Brand, Model, and Meta_Type from file paths.
5. IMPROVEMENT: Scans ALL folders to build a complete index for browsing.
6. NEW: `force_full_rescan_for_sync` to explicitly rescan ignored folders.
"""
import streamlit as st
import json
import os
from core.drive_manager import DriveManager
from core.config_loader import ConfigLoader
import logging
from googleapiclient.http import MediaIoBaseUpload
import io
import re

logger = logging.getLogger("Sync")
INDEX_FILENAME = "drive_index.json"

class SyncService:
    def __init__(self):
        self.drive = DriveManager()
        self.root_id = ConfigLoader.get_drive_folder_id()

    def scan_library(self):
        """Î£Î±ÏÏÎ½ÎµÎ¹ ÎºÎ±Î¹ Î•ÎÎ—ÎœÎ•Î¡Î©ÎÎ•Î™ (Update) Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ ÏƒÏ„Î¿ Cloud."""
        logger.info("ğŸ”„ Starting Sync (Direct Mode)...")
        
        if not self.root_id: 
            logger.error("âŒ Root ID missing.")
            return []
        
        # ÎœÏ€Î¬ÏÎ± Î ÏÎ¿ÏŒÎ´Î¿Ï…
        progress_text = "â³ Î£Î¬ÏÏ‰ÏƒÎ· & Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ·..."
        my_bar = st.progress(0, text=progress_text)
        
        # 1. Î£Î‘Î¡Î©Î£Î— (Path Aware)
        # Call _scan_recursive to scan all files, with force_full_rescan_for_sync=True
        # to ensure all folders (even those normally ignored by Sorter) are included in the index.
        all_files = self._scan_recursive(self.root_id, path_prefix="", my_bar=my_bar, progress_text=progress_text, current_progress=0, total_progress_steps=80, force_full_rescan_for_sync=True)
        
        my_bar.progress(80, text=f"âœ… Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(all_files)} Î±ÏÏ‡ÎµÎ¯Î±. Î•Î³Î³ÏÎ±Ï†Î® ÏƒÏ„Î¿ Cloud...")
        logger.info(f"âœ… Scan Complete. Found {len(all_files)} manuals.")

        # 2. Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î¤Î¿Ï€Î¹ÎºÎ¬ (Backup)
        try:
            with open(INDEX_FILENAME, "w", encoding="utf-8") as f:
                json.dump(all_files, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ Local index saved: {INDEX_FILENAME}")
        except Exception as e:
            logger.warning(f"Failed to save local index: {e}")

        # 3. CLOUD UPDATE (Direct API Call - Î§Ï‰ÏÎ¯Ï‚ Î¼ÎµÏƒÎ¬Î¶Î¿Î½Ï„ÎµÏ‚)
        try:
            # Î‘Ï€ÎµÏ…Î¸ÎµÎ¯Î±Ï‚ Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î¼Î­ÏƒÏ‰ Ï„Î¿Ï… service (Ï€Î±ÏÎ±ÎºÎ¬Î¼Ï€Ï„Î¿Ï…Î¼Îµ Ï„Î¿ DriveManager)
            query = f"name = '{INDEX_FILENAME}' and '{self.root_id}' in parents and trashed = false"
            results = self.drive.service.files().list(q=query, fields="files(id, name)").execute()
            found_files = results.get('files', [])
            
            if not found_files:
                logger.error("âŒ CLOUD ERROR: Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ Ï„Î¿ 'drive_index.json'!")
                st.error("âš ï¸ Î£Ï†Î¬Î»Î¼Î±: Î ÏÎ­Ï€ÎµÎ¹ Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î®ÏƒÎµÏ„Îµ Î­Î½Î± ÎºÎµÎ½ÏŒ Î±ÏÏ‡ÎµÎ¯Î¿ 'drive_index.json' ÏƒÏ„Î¿ Drive ÏƒÎ±Ï‚!")
                return all_files

            # Î Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î¿ ID Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï…
            target_file_id = found_files[0]['id']
            logger.info(f"ğŸ“‚ Found Cloud Index ID: {target_file_id}")
            
            # Î•Ï„Î¿Î¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±
            json_str = json.dumps(all_files, ensure_ascii=False, indent=2)
            media = MediaIoBaseUpload(io.BytesIO(json_str.encode('utf-8')), mimetype='application/json', resumable=True)

            # Î•ÎšÎ¤Î•Î›Î•Î£Î— UPDATE
            self.drive.service.files().update(
                fileId=target_file_id,
                media_body=media
            ).execute()
            
            logger.info(f"â˜ï¸ Cloud Index OVERWRITTEN successfully!")
            my_bar.progress(100, text="âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ! Î— Î²Î¬ÏƒÎ· ÎµÎ½Î·Î¼ÎµÏÏÎ¸Î·ÎºÎµ.")
            
            # ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Session
            if 'library_index' in st.session_state:
                del st.session_state['library_index']
            if 'library_cache' in st.session_state: # Clear cache from ui_search
                del st.session_state['library_cache']
                
            return all_files
            
        except Exception as e:
            logger.error(f"âŒ Cloud Update Failed: {e}", exc_info=True)
            st.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ· Cloud: {e}")
            return all_files

    def _extract_metadata_from_name(self, full_path_name: str, original_filename: str) -> dict:
        """
        Î•Î¾Î¬Î³ÎµÎ¹ Î¼ÎµÏ„Î±Î´ÎµÎ´Î¿Î¼Î­Î½Î± (category, brand, model, meta_type, error_codes) Î±Ï€ÏŒ Î­Î½Î± ÏŒÎ½Î¿Î¼Î± Î±ÏÏ‡ÎµÎ¯Î¿Ï…
        Ï€Î¿Ï… Î­Ï‡ÎµÎ¹ Î¼Î¿ÏÏ†Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Î±Ï€ÏŒ Ï„Î¿Î½ Sorter (Ï€.Ï‡., "Category | Brand | Model | Type | Filename.pdf")
        Î® Î±Ï€ÏŒ Ï„Î·Î½ original_filename Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Ï€Î»Î®ÏÎ·Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î®.
        """
        metadata = {
            'category': 'Unknown', # NEW: Include category in metadata
            'brand': 'Unknown',
            'model': 'General_Model', # Changed from 'General Model' for consistency
            'meta_type': 'General_Manual', # Changed from 'DOC' for consistency with Sorter
            'error_codes': '', 
            'original_name': original_filename 
        }

        # Î ÏÎ¿ÏƒÏ€Î±Î¸Î¿ÏÎ¼Îµ Î½Î± Î´Î¹Î±ÏƒÏ€Î¬ÏƒÎ¿Ï…Î¼Îµ Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î¿ '|' Î±Ï€ÏŒ Ï„Î·Î½ Ï€Î»Î®ÏÎ· Î´Î¹Î±Î´ÏÎ¿Î¼Î®
        parts = [p.strip() for p in full_path_name.split('|')]
        
        # Î‘Î½ Î· Ï€Î»Î®ÏÎ·Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î® ÎµÎ¯Î½Î±Î¹ Î¼Î¿ÏÏ†Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î·
        if len(parts) >= 2:
            metadata['category'] = parts[0]
            metadata['brand'] = parts[1]
            if len(parts) >= 3:
                metadata['model'] = parts[2]
            if len(parts) >= 4:
                metadata['meta_type'] = parts[3]
            
            # Try to extract error codes if they appear in original filename
            error_code_match = re.search(r'[E|F|P|C][0-9]{1,3}', original_filename, re.IGNORECASE)
            if error_code_match:
                metadata['error_codes'] = error_code_match.group(0).upper()
            
        # Fallback for simpler filenames or those not yet sorted
        else:
            # Attempt to parse brand from the start of the original filename
            brand_match = re.match(r'([A-Za-z0-9_]+)', original_filename)
            if brand_match:
                metadata['brand'] = brand_match.group(1).replace('_', ' ').strip()
            
            # Also try to get error codes from original name if path is not formatted
            error_code_match = re.search(r'[E|F|P|C][0-9]{1,3}', original_filename, re.IGNORECASE)
            if error_code_match:
                metadata['error_codes'] = error_code_match.group(0).upper()

        return metadata

    def _scan_recursive(self, folder_id, path_prefix="", my_bar=None, progress_text="", current_progress=0, total_progress_steps=100, force_full_rescan_for_sync=False):
        """
        Î£Î±ÏÏÎ½ÎµÎ¹ Î±Î½Î±Î´ÏÎ¿Î¼Î¹ÎºÎ¬ Ï†Î±ÎºÎ­Î»Î¿Ï…Ï‚ ÏƒÏ„Î¿ Google Drive, ÎµÎ¾Î¬Î³Î¿Î½Ï„Î±Ï‚ Î¼ÎµÏ„Î±Î´ÎµÎ´Î¿Î¼Î­Î½Î±.
        `force_full_rescan_for_sync`: Î‘Î½ ÎµÎ¯Î½Î±Î¹ True, Î´ÎµÎ½ Î±Î³Î½Î¿ÎµÎ¯ Ï†Î±ÎºÎ­Î»Î¿Ï…Ï‚ ÏŒÏ€Ï‰Ï‚ `_MANUAL_REVIEW`, `_IRRELEVANT_OR_UNKNOWN`, ÎºÎ»Ï€.
        """
        files_data = []
        try:
            items = self.drive.list_files_in_folder(folder_id)
            for i, item in enumerate(items):
                # Update progress bar
                if my_bar:
                    step_progress = (i + 1) / len(items) if len(items) > 0 else 0
                    total_progress = current_progress + (step_progress * total_progress_steps) / 100
                    my_bar.progress(min(int(total_progress), 100), text=f"{progress_text} {path_prefix}{item['name']}")

                item_name = item['name']
                
                # Check if it's a folder
                if item['mimeType'] == 'application/vnd.google-apps.folder':
                    # Only ignore if not forced to rescan AND the folder is in the ignored list
                    if not force_full_rescan_for_sync and item_name in IGNORED_FOLDERS_TOP_LEVEL:
                        logger.debug(f"Skipping ignored folder: {path_prefix}{item_name}")
                        continue
                    
                    # Recursive call
                    nested_files = self._scan_recursive(item['id'], path_prefix=f"{path_prefix}{item_name} | ", my_bar=my_bar, progress_text=progress_text, current_progress=total_progress, total_progress_steps=total_progress_steps, force_full_rescan_for_sync=force_full_rescan_for_sync)
                    files_data.extend(nested_files)
                else:
                    # It's a file
                    full_path_name = f"{path_prefix}{item_name}"
                    # Use original_name for better metadata extraction
                    original_name = item_name # At this stage, item_name is the original name
                    metadata = self._extract_metadata_from_name(full_path_name, original_name)
                    
                    files_data.append({
                        "file_id": item['id'],
                        "name": full_path_name, # The full path including category, brand, model
                        "link": item['webViewLink'],
                        "mime": item['mimeType'],
                        **metadata # Unpack the extracted metadata
                    })
        except Exception as e:
            logger.error(f"Error scanning folder {folder_id} ('{path_prefix}'): {e}", exc_info=True)
        
        return files_data

    @st.cache_data(ttl=3600, show_spinner="Î¦ÏŒÏÏ„Ï‰ÏƒÎ· ÎµÏ…ÏÎµÏ„Î·ÏÎ¯Î¿Ï… Manuals Î±Ï€ÏŒ Ï„Î¿ Drive...")
    def load_index(_self): # Use _self to mark it as instance method, not class method for Streamlit cache
        """Î¦Î¿ÏÏ„ÏÎ½ÎµÎ¹ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ json Î±Ï€ÏŒ Ï„Î¿ Drive Î® Î±Ï€ÏŒ Ï„Î¿Î½ Ï„Î¿Ï€Î¹ÎºÏŒ Î´Î¯ÏƒÎºÎ¿."""
        # logger.info("Attempting to load index from Drive.")
        try:
            # 1. Î ÏÏÏ„Î± Î´Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Ï…Î¼Îµ Î½Î± Ï„Î¿ ÎºÎ±Ï„ÎµÎ²Î¬ÏƒÎ¿Ï…Î¼Îµ Î±Ï€ÏŒ Ï„Î¿ Drive
            query = f"name = '{INDEX_FILENAME}' and '{_self.root_id}' in parents and trashed = false"
            results = _self.drive.service.files().list(q=query, fields="files(id, name)").execute()
            found_files = results.get('files', [])

            if found_files:
                target_file_id = found_files[0]['id']
                file_stream = _self.drive.download_file_content(target_file_id)
                if file_stream:
                    data = json.load(file_stream)
                    logger.info(f"âœ… Loaded index from Google Drive: {INDEX_FILENAME}")
                    return data
            
            logger.warning(f"'{INDEX_FILENAME}' not found in Google Drive. Attempting local load.")

        except Exception as e:
            logger.error(f"Error loading index from Google Drive: {e}", exc_info=True)
            logger.warning(f"Could not load index from Drive. Attempting local load from {INDEX_FILENAME}.")

        # 2. Fallback: Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î±Ï€ÏŒ Ï„Î¿Ï€Î¹ÎºÏŒ Î±ÏÏ‡ÎµÎ¯Î¿ Î±Î½ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î¿ Drive Î® Î±Ï€Î­Ï„Ï…Ï‡Îµ Î· Î»Î®ÏˆÎ·
        if os.path.exists(INDEX_FILENAME):
            try:
                with open(INDEX_FILENAME, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    logger.info(f"ğŸ’¾ Loaded local index from: {INDEX_FILENAME}")
                    return data
            except Exception as e:
                logger.error(f"Error loading local index: {e}", exc_info=True)
        
        logger.warning("No index found locally or on Drive. Returning empty list.")
        return []